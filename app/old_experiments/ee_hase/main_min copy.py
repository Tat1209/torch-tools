import sys
from pathlib import Path
import math

import torch
from torchvision import transforms

work_path = Path(next((p for p in Path(__file__).resolve().parents if p.name == "Research"), None))
torchlib_path = str(work_path / Path("app/torch_libs"))
sys.path.append(torchlib_path)

from datasets import Datasets
from trainer import Trainer, MultiTrainer
from trans import Trans

from mytrainer import MyTrainer

# from models.resnet_ee import resnet18 as net
from models.gitresnet_ee import resnet18 as net

ds = Datasets(root=work_path / "assets/datasets/")

device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")

max_lr = 0.1
batch_size = 128

epochs = 100

fils = 64
ensembles = 1

# 汎用データ拡張
# train_trans = [transforms.ToTensor(), transforms.RandomHorizontalFlip(p=0.5), transforms.RandomRotation(degrees=(0, 360))] 
# val_trans = [transforms.ToTensor()]

# cifar100
train_trans = [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.RandomRotation(15), transforms.ToTensor(), transforms.Normalize(mean=[0.5070751905441284, 0.48654890060424805, 0.44091784954071045], std=[0.2673342823982239, 0.2564384639263153, 0.2761504650115967], inplace=True)]
val_trans = [transforms.ToTensor(), transforms.Normalize(mean=[0.5070751905441284, 0.48654890060424805, 0.44091784954071045], std=[0.2673342823982239, 0.2564384639263153, 0.2761504650115967], inplace=True)]

# stl10
# train_trans = [transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.RandomRotation(15), transforms.ToTensor(), transforms.Normalize(mean=[0.5070751905441284, 0.48654890060424805, 0.44091784954071045], std=[0.2673342823982239, 0.2564384639263153, 0.2761504650115967], inplace=True)]
# val_trans = [transforms.ToTensor(), transforms.Normalize(mean=[0.44671064615249634, 0.4398098886013031, 0.4066464304924011], std=[0.26034098863601685, 0.2565772831439972, 0.2712673842906952], inplace=True)]

train_ds = ds("cifar100_train", train_trans) # 第1引数に対応するデータセットを取得 対応する文字列はDatasetsクラスを参照
val_ds = ds("cifar100_val", val_trans)
# train_ds = ds("stl10_train", train_trans)
# val_ds = ds("stl10_val", val_trans)

train_dl = train_ds.loader(batch_size=batch_size)
val_dl = val_ds.loader(batch_size=batch_size)

# network = net(num_classes=train_ds.fetch_classes(), nb_fils=fils, ee_groups=ensembles)
# loss_func = torch.nn.CrossEntropyLoss()
# optimizer = torch.optim.Adam(network.parameters(), lr=max_lr)
# scheduler_t = (torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0, last_epoch=-1), "epoch")

# trainer = Trainer(network, loss_func, optimizer, scheduler_t, device)

# # 訓練 lossやaccを出力するならループ内のコメントアウトを解除
# for e in range(epochs):
#     train_loss, train_acc = trainer.train_1epoch(train_dl)
#     met_dict = {"epoch": e + 1, "train_loss": train_loss, "train_acc": train_acc}
#     val_loss, val_acc = trainer.val_1epoch(val_dl)
#     met_dict.update({"val_loss": val_loss, "val_acc": val_acc})

#     trainer.printmet(met_dict, e + 1, epochs, itv=epochs / 4, timezone=0) # 仕様は実装コードに



# 単一loaderで複数のモデルを訓練するサンプル

# trainers = []
# fils_l = [64, 16]
# ensembles_l = [1, 16]

# for fils, ensembles in zip(fils_l, ensembles_l):
#     network = net(num_classes=train_ds.fetch_classes(), nb_fils=fils, ee_groups=ensembles)

#     loss_func = torch.nn.CrossEntropyLoss()
#     optimizer = torch.optim.Adam(network.parameters(), lr=max_lr)
#     scheduler_t = (torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0, last_epoch=-1), "epoch")

#     trainer = Trainer(network, loss_func, optimizer, scheduler_t, device)
#     trainers.append(trainer)
# mtrainer = MultiTrainer(trainers, device)

# for e in range(epochs):
#     train_loss, train_acc = mtrainer.train_1epoch(train_dl) # それぞれ値が格納されたリストがreturnされる
#     met_dict = {"epoch": e + 1, "train_loss": train_loss, "train_acc": train_acc}

#     val_loss, val_acc = mtrainer.val_1epoch(val_dl)
#     met_dict.update({"val_loss": val_loss, "val_acc": val_acc})

#     mtrainer.printmet(met_dict, e + 1, epochs, itv=epochs / 4, anyval=True)


trainers = []
max_lr_l = [0.1]

for max_lr in max_lr_l:
    network = net(num_classes=train_ds.fetch_classes(), nb_fils=fils, ee_groups=ensembles)

    loss_func = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(network.parameters(), lr=max_lr)
    scheduler_t = (torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0, last_epoch=-1), "epoch")

    trainer = Trainer(network, loss_func, optimizer, scheduler_t, device)
    trainers.append(trainer)
mtrainer = MultiTrainer(trainers, device)

for e in range(epochs):
    train_loss, train_acc = mtrainer.train_1epoch(train_dl) # それぞれ値が格納されたリストがreturnされる
    met_dict = {"epoch": e + 1, "train_loss": train_loss, "train_acc": train_acc}

    val_loss, val_acc = mtrainer.val_1epoch(val_dl)
    met_dict.update({"val_loss": val_loss, "val_acc": val_acc})

    mtrainer.printmet(met_dict, e + 1, epochs, itv=epochs / 4, anyval=True)
