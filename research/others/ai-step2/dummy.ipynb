{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Wa47OlJTBJDr"
   },
   "source": [
    "# Lecture 2\n",
    "こちらは __宇宙ビッグデータの画像__ を7つのクラス「Bare (裸地), Built-up (建物), Cropland (農地), Forest (森), Grassland (草原), Paddy_field (水田), Water_bodies (水域)」に分類するプログラムのサンプルです．\n",
    "内容については，Lecture1とほとんど同じですので，こちらも __講義で説明した改良部分 (ハイパーパラメータ，モデル構造)__を変更し，コンペティションで上位を目指しましょう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "jKry6iQlBJDt"
   },
   "outputs": [],
   "source": [
    "# プログラムで利用する各種パッケージの定義です\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, ReLU, Softmax\n",
    "from keras.saving import save_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import F1Score\n",
    "from keras.utils import to_categorical, plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "knoBwIZpRPku",
    "outputId": "9c508bf8-cf21-4d78-d100-7a9b7b0541b7"
   },
   "outputs": [],
   "source": [
    "# # サーバから宇宙ビッグデータをダウンロードします\n",
    "!wget -N --http-user=aistepkanazawa --http-password=spaceai \"https://ai-step.ec.t.kanazawa-u.ac.jp/competition_02/data/competition02_32_60_7_dataset.zip\"\n",
    "!unzip -o -q \"competition02_32_60_7_dataset.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WW3cJ-WggTLt"
   },
   "outputs": [],
   "source": [
    "# 訓練データ，テストデータのファイル名を設定\n",
    "base_dir = \"\"\n",
    "train_pkl = os.path.join(base_dir, \"fukui_train_32_60_ver2.pkl\")\n",
    "test_pkl = os.path.join(base_dir, \"kanazawa_test_32_60_ver2.pkl\")\n",
    "\n",
    "# 出力ファイル名\n",
    "dt_str = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "model_name = \"model_{}.keras\".format(dt_str)\n",
    "result_csv_name = \"result_csv_{}.csv\".format(dt_str)\n",
    "result_img_name = \"result_img_{}.png\".format(dt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MZujX-AZBJDu"
   },
   "outputs": [],
   "source": [
    "# 画像のサイズ (横幅) 単位：ピクセル\n",
    "IMG_WIDTH = 32\n",
    "# 画像のサイズ (縦幅) 単位：ピクセル\n",
    "IMG_HEIGHT = 32\n",
    "\n",
    "# 今回は7種類に分類を行います (classesには「7」が入る)\n",
    "#   Bare (裸地), Built-up (建物), Cropland (農地), Forest (森), Grassland (草原), Paddy_field (水田), Water_bodies (水域)\n",
    "classes = 7\n",
    "\n",
    "# モデルが出力する分類結果 (数値)と，名前を紐づける\n",
    "#   Bare = 0, Built-up = 1, Cropland = 2, Forest = 3, Grassland = 4, Paddy_field = 5, Water_bodies = 6\n",
    "class_names = [\"Bare\", \"Built-up\", \"Cropland\", \"Forest\", \"Grassland\", \"Paddy_field\", \"Water_bodies\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "NvrdbgTaBJDv"
   },
   "source": [
    "## ハイパーパラメータの設定\n",
    "こちらは，学習に用いる各種パラメータ (ハイパーパラメータと呼ばれる)を設定しています．\n",
    "\n",
    "- **バッチサイズ**\n",
    "    - 主に**2のn乗**の値を設定します\n",
    "    - こちらの値が大きい方が並列で学習しますので，速度が向上します\n",
    "        - 大きくしすぎると，メモリが足りなくなってエラー終了しますので注意してください\n",
    "    - 例：2, 4, 8, 16, 32, ...\n",
    "- **エポック数**\n",
    "    - 繰り返し学習をする回数になります．精度が落ち着くまでの回数を指定しましょう．\n",
    "    - 1エポックは，すべての学習データを使って1回学習しています．\n",
    "- **学習率**\n",
    "    - 重みをどの程度の大きさで変更するか？を指定する値になります．\n",
    "    - 主に0.1以下で，0.01，0.001など，非常に小さな値を指定します\n",
    "    - 学習を進めても精度が変わらない場合は，こちらの値を増減させてみてください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0s4d9DLzBJDv"
   },
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "# こちらの項目は「ハイパーパラメータ」と呼ばれる項目になります\n",
    "# 値を変更してモデルの精度を向上させてみましょう！\n",
    "###################################################################################\n",
    "# バッチサイズ (並列して学習を実施する数)\n",
    "batch_size = 256\n",
    "\n",
    "# エポック数 (学習を何回実施するか？という変数)\n",
    "epochs = 20\n",
    "\n",
    "# 学習率 (重みをどの程度変更するか？)\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kfFH2tIqgFH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qc3OrKLORPkv"
   },
   "outputs": [],
   "source": [
    "# 訓練データと検証データに分割を行う関数\n",
    "def train_val_split(x, y, ratio=0.7, shuffle=False):\n",
    "    train_idx_list = []\n",
    "    val_idx_list = []\n",
    "\n",
    "    for i in range(classes):\n",
    "        all_data = np.where(y == i)[0]\n",
    "        train_idx, val_idx = np.split(all_data, [int(all_data.size * ratio)])\n",
    "        train_idx_list.extend(train_idx)\n",
    "        val_idx_list.extend(val_idx)\n",
    "\n",
    "    # シャッフルの指定があれば，データをシャッフルする\n",
    "    if shuffle:\n",
    "        np.random.shuffle(train_idx_list)\n",
    "        np.random.shuffle(val_idx_list)\n",
    "\n",
    "    return (np.array(x[train_idx_list], dtype=np.float32),\n",
    "            np.array(y[train_idx_list], dtype=np.uint8),\n",
    "            np.array(x[val_idx_list], dtype=np.float32),\n",
    "            np.array(y[val_idx_list], dtype=np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EC7JpYdjRPkv"
   },
   "outputs": [],
   "source": [
    "# 訓練データを読み込みます\n",
    "with open(train_pkl, 'rb') as f:\n",
    "    (train_img, train_label) = pickle.load(f)\n",
    "\n",
    "# 訓練データと検証データに分割を行います\n",
    "#  ratio=0.7で，訓練データの割合は70%としています\n",
    "#  shuffle=Falseで，データのシャッフルは行いません (毎回同じデータが訓練・検証データに分割されます)\n",
    "train_x, train_y, val_x, val_y = train_val_split(train_img, train_label, ratio=0.7, shuffle=False)\n",
    "\n",
    "# データの前処理\n",
    "train_x = train_x / 255.0\n",
    "train_y = to_categorical(train_y, classes)\n",
    "\n",
    "# データの前処理\n",
    "val_x = val_x / 255.0\n",
    "val_y = to_categorical(val_y, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "My6Hk1eOBJDw"
   },
   "source": [
    "## モデル構造 (特徴抽出器)の定義\n",
    "モデルの構造について，こちらで定義しています．\n",
    "基本的には，以下の順序で層を重ねることでモデルを構築していきます．\n",
    "\n",
    "### 各層について\n",
    "- __畳み込み層 (Conv2D)__\n",
    "    - filters  ：出力するフィルタ (チャネル)の枚数を指定します\n",
    "    - kernel_size  ：畳み込みのカーネルサイズを指定します (奇数にすることがほとんどです 3, 5, 7, ...)\n",
    "- __活性化関数 (ReLU)__\n",
    "    - 活性化関数はいくつかありますが，畳み込みのあとは「ReLU」関数で問題ないでしょう\n",
    "- __プーリング層 (MaxPool2D)__\n",
    "    -  __2__ を指定すると，画像サイズが1/2になり， __4__ を指定すると，画像サイズが1/4になります\n",
    "    - 基本的には __2__ のみを指定することが多いです\n",
    "\n",
    "### 層の組み合わせ\n",
    "以下のように組み合わせて構築することが多いです．\n",
    "**畳み込みと活性化関数は，必ずセットで**利用するようにしましょう\n",
    "- 畳み込み - 活性化関数 - プーリング層\n",
    "- 畳み込み - 活性化関数 - 畳み込み - 活性化関数 - プーリング層\n",
    "- 畳み込み - 活性化関数 - ... - プーリング層\n",
    "\n",
    "### 畳み込みのフィルタ (チャネル)とプーリングの関係\n",
    "また，プーリング層を挟んで画像のサイズを半分にすると，その次から畳み込みのフィルタ数を2倍にするのが基本的な設定です．\n",
    "\n",
    "    Conv2D(filters=32)\n",
    "    ReLU()\n",
    "    MaxPool2D(2)\n",
    "\n",
    "    Conv2D(filters=64)\n",
    "    ReLU()\n",
    "    MaxPool2D(2)\n",
    "\n",
    "    Conv2D(filters=128)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RO-BnLlUBJDw"
   },
   "outputs": [],
   "source": [
    "# モデルの構築\n",
    "#  この「model」という変数に，構築するモデルのすべての情報が入ります\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# モデルの編集 (特徴抽出器)\n",
    "# 編集場所はここから！\n",
    "###################################################################################\n",
    "# 畳み込み - 活性化関数 (ReLU) - プーリング\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same'))\n",
    "model.add(ReLU())\n",
    "model.add(MaxPool2D(2))\n",
    "\n",
    "# 畳み込み - 活性化関数 (ReLU) - プーリング\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
    "model.add(ReLU())\n",
    "model.add(MaxPool2D(2))\n",
    "###################################################################################\n",
    "# ここまで！\n",
    "\n",
    "# 分類器 (こちらは編集しない！)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(classes))\n",
    "model.add(Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "F9IAEFw4BJDw",
    "outputId": "512520eb-76bb-4733-cc44-1350eddb9ddf"
   },
   "outputs": [],
   "source": [
    "# 損失関数 (1 - macro-F1を用いて損失を計算する)\n",
    "@tf.function\n",
    "def macro_soft_f1(y, y_pred):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    tp = tf.reduce_sum(y_pred * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_pred * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_pred) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1\n",
    "    loss = tf.reduce_mean(cost)\n",
    "    return loss\n",
    "\n",
    "# 最適化関数の設定\n",
    "#  \"パラメータをどのように更新していくか？\"という設定項目になります (学習率をこちらで使っています)\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# 学習が実施できるように，モデルの設定を完了します\n",
    "#  loss=macro_soft_f1 で，損失関数として 1 - macro-F1を設定しています\n",
    "#  metricsに「F1Score」を設定して，macro-F1で評価した結果を出力します\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=macro_soft_f1,\n",
    "              metrics=[\"accuracy\", F1Score(average=\"macro\", threshold=0.5)])\n",
    "\n",
    "model.build(input_shape=(batch_size, IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "# 確認のため，モデルの構造を表示してみます\n",
    "plot_model(model, show_shapes=True, expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nzb-eBanBJDx",
    "outputId": "936785d4-bf54-47a9-e077-2600379bc336"
   },
   "outputs": [],
   "source": [
    "# 学習を実施します\n",
    "#  訓練データと検証データをそれぞれ設定しています\n",
    "history = model.fit(x=train_x, y=train_y, validation_data=(val_x, val_y),\n",
    "                    epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# モデルの保存を行います\n",
    "#   ファイル名は「model_(日時).keras」として，保存されます\n",
    "save_model(model, model_name, overwrite=True, save_format=\"keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "jOlXnJw4RPkx"
   },
   "source": [
    "以下は，コンペティションで提出するデータを作成するプログラムです．\n",
    "また，テストデータに対して，土地被覆図の画像についても作成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fql0dTivRPkx"
   },
   "outputs": [],
   "source": [
    "# 混同行列と，macro-F1を出力する関数です\n",
    "def create_cm_f1(x, y_true):\n",
    "    y_pred = []\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        # データをバッチ化して保存\n",
    "        batch_img = x[i:i+batch_size]\n",
    "\n",
    "        # 予測の実施\n",
    "        predictions = model.predict(batch_img, verbose=0)\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # 結果をリストに保存\n",
    "        y_pred.extend(predicted_classes)\n",
    "\n",
    "    # データの整形\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if y_true.ndim != 1:\n",
    "        y_true = np.asarray(y_true).argmax(axis=1)\n",
    "\n",
    "    # 混同行列を表示\n",
    "    cm = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=list(range(0, classes)))\n",
    "\n",
    "    cm_df = pd.DataFrame(data=cm, index=class_names, columns=class_names)\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap=\"Blues\")\n",
    "\n",
    "    macro_f1 = precision_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0.0)\n",
    "    print(\"F1-Score (macro-F1) = {}\".format(macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "VWc9U-2ERPkx",
    "outputId": "53bd071e-e94c-4c73-c6ee-c0142adcdc6a"
   },
   "outputs": [],
   "source": [
    "# 検証データに対する混同行列を作成して，macro-F1を確認する\n",
    "create_cm_f1(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "P0tJX9fiRPkx",
    "outputId": "bc43345f-3e33-418e-fca6-3d676555942c"
   },
   "outputs": [],
   "source": [
    "# 検証データに対する混同行列を作成して，macro-F1を確認する\n",
    "create_cm_f1(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3P6XF0fLudRt"
   },
   "outputs": [],
   "source": [
    "# テストデータを読み込んで実施してみる\n",
    "with open(test_pkl, 'rb') as f:\n",
    "    (test_img, file_name_list) = pickle.load(f)\n",
    "# テストデータの前処理\n",
    "test_img = test_img / 255.0\n",
    "\n",
    "data_list = []\n",
    "# バッチごとに処理して，CSVを作成する\n",
    "for i in range(0, len(test_img), batch_size):\n",
    "    # データをバッチ化して保存\n",
    "    batch_img = test_img[i:i+batch_size]\n",
    "    batch_file_names = file_name_list[i:i+batch_size]\n",
    "\n",
    "    # 予測の実施\n",
    "    predictions = model.predict(batch_img, verbose=0)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # 結果をリストに保存\n",
    "    batch_result = list(zip(batch_file_names, predicted_classes))\n",
    "    data_list.extend(batch_result)\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "#################################################################################\n",
    "# テストデータに対する分類結果をCSVファイルに保存します\n",
    "#  ファイル名は，「result_csv_(日時).csv」となります\n",
    "#  このファイルを提出してください！！！\n",
    "#################################################################################\n",
    "df.to_csv(result_csv_name, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "OJfHbykaRPkx"
   },
   "source": [
    "テストデータに対する分類結果を利用して，土地被覆図を作成して表示してみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "r9nJjo0RRPkx",
    "outputId": "956febd6-eb83-47ba-afbb-edcb06466c8b"
   },
   "outputs": [],
   "source": [
    "# 土地被覆図の画素について辞書作成 (BGR)\n",
    "dic = {\n",
    "    0: (0, 100, 128),\n",
    "    1: (0, 0, 255),\n",
    "    2: (191, 193, 255),\n",
    "    3: (0, 128, 0),\n",
    "    4: (0, 255, 255),\n",
    "    5: (255, 128, 0),\n",
    "    6: (100, 0, 0),\n",
    "}\n",
    "\n",
    "cell_size = 32\n",
    "\n",
    "# CSVファイルを読み込む\n",
    "df = pd.read_csv(result_csv_name, header=None)\n",
    "\n",
    "ref_file = df[0][0]\n",
    "# グリッドのサイズを取得\n",
    "cols = int(re.findall(r\"\\d+\", str(ref_file))[-3])\n",
    "rows = int(re.findall(r\"\\d+\", str(ref_file))[-4])\n",
    "\n",
    "# グリッド画像のサイズを計算\n",
    "width = cell_size * cols\n",
    "height = cell_size * rows\n",
    "\n",
    "# 空の画像を作成（白色の背景）\n",
    "inference_img = np.ones((height, width, 3), dtype=np.uint8) * 255\n",
    "\n",
    "# 各セルにデフォルトの値を設定\n",
    "default_img = np.ones((cell_size, cell_size, 3), dtype=np.uint8) * 255\n",
    "\n",
    "for i in range(len(df)):\n",
    "    filename = df.iloc[i, 0]\n",
    "    cell_value = int(df.iloc[i, 1])\n",
    "\n",
    "    match = re.search(r'_(\\d+)_(\\d+).png$', filename)\n",
    "    if match:\n",
    "        row = int(match.group(1))\n",
    "        col = int(match.group(2))\n",
    "\n",
    "        if cell_value in dic:\n",
    "          img = np.ones((cell_size, cell_size, 3), dtype=np.uint8) * 255\n",
    "          img[:,:] = dic[cell_value]\n",
    "\n",
    "        else:\n",
    "          img = default_img\n",
    "\n",
    "        inference_img[row * cell_size:(row + 1) * cell_size, col * cell_size:(col + 1) * cell_size] = img\n",
    "\n",
    "img_rgb = inference_img[:, :, [2, 1, 0]]\n",
    "\n",
    "#################################################################################\n",
    "# 画像を保存します\n",
    "#  ファイル名は，「result_img_(日時).png」となります\n",
    "#################################################################################\n",
    "pil_image = Image.fromarray(img_rgb)\n",
    "pil_image.save(result_img_name)\n",
    "\n",
    "# 結果の表示\n",
    "ax = plt.gca()\n",
    "ax.axes.xaxis.set_visible(False)\n",
    "ax.axes.yaxis.set_visible(False)\n",
    "plt.imshow(pil_image)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
